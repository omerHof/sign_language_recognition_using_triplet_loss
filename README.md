# Sign Language Recognition using Triplet Loss (SLR-TL)

Sign language recognition task have recently gained popularity in the research community as a communication solution with deaf people. 
Despite this, some challenges in the field remain unanswered, particularly those related to scalability. This is reflected in the methods designed to classify only a limited number of signs and in addition, the relatively small datasets from which a model can learn would be problematic to apply in practice. Our paper presents a new method that was originally developed for the face recognition domain but can also be applied to the recognition of sign language. Over a new dataset, we assemble and which contains three times more examples per class (sign), we compare the proposed method with state-of-the-art methods. In addition, we identified two key parameters in the sign recognition domain and demonstrated through an ablation study that they directly affect the model's performance. Our results show that the proposed method outperformed the existing methods, achieving 98.6\% accuracy, and can identify new signs that the model has not been trained on. 
